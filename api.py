from fastapi import FastAPI, HTTPException, Response, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
# from transformers.pipelines import pipeline
# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
# import torch
import re
import json
import os
import asyncio
from pathlib import Path
from typing import Dict, List, Optional
from groq import AsyncGroq
from dotenv import load_dotenv
import random
import whisper
from gtts import gTTS
import tempfile

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

app = FastAPI()

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
    expose_headers=["*"]
)

# ëª¨ë¸ ì´ˆê¸°í™”
print("Starting MBTI T/F Analyzer...")

# AI ëª¨ë¸ ì´ˆê¸°í™” (Groq)
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "")
if GROQ_API_KEY:
    AI_CLIENT = AsyncGroq(api_key=GROQ_API_KEY)
    print("Groq AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ", flush=True)
else:
    AI_CLIENT = None
    print("âš ï¸ GROQ_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ. AI ì§ˆë¬¸ ìƒì„±ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.", flush=True)

# STT ëª¨ë¸ ì´ˆê¸°í™”
print("Loading Whisper model...")
whisper_model = whisper.load_model("base")
print("Whisper model loaded successfully!")

# Main_pg, images í´ë”ë¥¼ static íŒŒì¼ë¡œ ì„œë¹„ìŠ¤
app.mount("/Main_pg", StaticFiles(directory="Main_pg"), name="mainpg")
app.mount("/images", StaticFiles(directory="images"), name="images")

class TextRequest(BaseModel):
    text: str

class DetailedAnalysisRequest(BaseModel):
    question: str
    answer: str
    score: float

class AnalysisResponse(BaseModel):
    score: float
    detailed_analysis: Optional[str] = None
    reasoning: Optional[str] = None
    suggestions: Optional[list] = None
    alternative_response: Optional[str] = None

class FinalAnalysisRequest(BaseModel):
    results: List[Dict]  # [{question, answer, score}, ...]

class FinalAnalysisResponse(BaseModel):
    overall_tendency: str
    personality_analysis: str
    communication_strategy: str
    strengths: List[str]
    growth_areas: List[str]
    keyword_analysis: Dict[str, Dict[str, int]]  # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ì‚¬ìš© íšŸìˆ˜

class QuestionGenerationRequest(BaseModel):
    count: Optional[int] = 5  # ìƒì„±í•  ì§ˆë¬¸ ê°œìˆ˜
    difficulty: Optional[str] = "medium"  # easy, medium, hard

async def generate_ai_questions_real(count: int = 5, difficulty: str = "medium") -> List[str]:
    """
    ì‹¤ì œ AIë¥¼ ì‚¬ìš©í•˜ì—¬ T/F ì„±í–¥ ë¶„ì„ì„ ìœ„í•œ ì§ˆë¬¸ë“¤ì„ ë™ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not AI_CLIENT:
        print("âŒ AI ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ. í´ë°± ì§ˆë¬¸ ì‚¬ìš©.")
        return generate_fallback_questions(count)
    
    try:
        # ë‚œì´ë„ë³„ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        difficulty_prompts = {
            "easy": "ì¼ìƒì ì´ê³  ê°€ë²¼ìš´ ìƒí™©ì—ì„œ",
            "medium": "ì•½ê°„ ë³µì¡í•˜ê³  ê³ ë¯¼ì´ í•„ìš”í•œ ìƒí™©ì—ì„œ", 
            "hard": "ë³µì¡í•˜ê³  ì–´ë ¤ìš´ ë”œë ˆë§ˆ ìƒí™©ì—ì„œ"
        }
        
        difficulty_desc = difficulty_prompts.get(difficulty, difficulty_prompts["medium"])
        
        prompt = f"""
        MBTI T/F ì„±í–¥ì„ ë¶„ì„í•˜ê¸° ìœ„í•œ ìƒí™© ì§ˆë¬¸ì„ {count}ê°œ ìƒì„±í•´ì¤˜.

        ìš”êµ¬ì‚¬í•­:
        1. {difficulty_desc} ì–´ë–»ê²Œ ëŒ€ì‘í• ì§€ ë¬»ëŠ” ì§ˆë¬¸
        2. ê´€ê³„, ì†Œí†µ, ê°ˆë“± í•´ê²°, ì˜ì‚¬ê²°ì • ìƒí™© ì¤‘ì‹¬
        3. Tì„±í–¥(ë…¼ë¦¬ì /ê°ê´€ì )ê³¼ Fì„±í–¥(ê°ì •ì /ê´€ê³„ì¤‘ì‹¬) êµ¬ë¶„ì´ ê°€ëŠ¥í•œ ìƒí™©
        4. ê° ì§ˆë¬¸ì€ \"ë‹¹ì‹ ì´ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì–´ë–»ê²Œ ëŒ€ì‘í• ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”\" í˜•íƒœë¡œ ëë‚˜ì•¼ í•¨
        5. ì‹¤ì œ ì¼ìƒì—ì„œ ê²ªì„ ìˆ˜ ìˆëŠ” í˜„ì‹¤ì ì¸ ìƒí™©ë“¤
        6. í•œêµ­ì–´ë¡œ ì‘ì„±, ì¡´ëŒ“ë§ ì‚¬ìš©

        ì˜ˆì‹œ í˜•íƒœ:
        \"ì¹œêµ¬ê°€ 'ìš”ì¦˜ ë„ˆë¬´ í˜ë“¤ì–´'ë¼ê³  í„¸ì–´ë†“ì•˜ìŠµë‹ˆë‹¤. ë‹¹ì‹ ì´ ì–´ë–¤ ë§ˆìŒìœ¼ë¡œ ì–´ë–»ê²Œ ëŒ€ì‘í• ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"

        {count}ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ìƒí™© ì§ˆë¬¸ì„ ìƒì„±í•´ì¤˜. ê° ì§ˆë¬¸ì€ ë²ˆí˜¸ ì—†ì´ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•´ì¤˜.
        """
        
        response = await AI_CLIENT.chat.completions.create(
            messages=[
                {"role": "user", "content": prompt}
            ],
            model="llama3-8b-8192",
        )
        questions_text = response.choices[0].message.content
        if questions_text is not None:
            questions_text = questions_text.strip()
        else:
            questions_text = ""
        
        # ìƒì„±ëœ ì§ˆë¬¸ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„í• 
        questions = [q.strip() for q in questions_text.split('\n') if q.strip()]
        
        # ë¹ˆ ì§ˆë¬¸ì´ë‚˜ í˜•ì‹ì´ ë§ì§€ ì•ŠëŠ” ì§ˆë¬¸ í•„í„°ë§
        valid_questions = []
        for q in questions:
            # ë²ˆí˜¸ë‚˜ ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
            q = re.sub(r'^\d+[\.\)\-\s]*', '', q)
            q = re.sub(r'^[\-\*\â€¢]\s*', '', q)
            q = q.strip()
            
            if len(q) > 20 and 'ë‹¹ì‹ ì´' in q and ('ì–´ë–»ê²Œ' in q or 'ì–´ë–¤' in q):
                valid_questions.append(q)
        
        # ìš”ì²­ëœ ê°œìˆ˜ë§Œí¼ ë°˜í™˜
        if len(valid_questions) >= count:
            return valid_questions[:count]
        else:
            # ë¶€ì¡±í•˜ë©´ í´ë°± ì§ˆë¬¸ìœ¼ë¡œ ì±„ì›€
            fallback = generate_fallback_questions(count - len(valid_questions))
            return valid_questions + fallback
            
    except Exception as e:
        print(f"âŒ AI ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return generate_fallback_questions(count)

def generate_fallback_questions(count: int = 5) -> List[str]:
    """
    AI ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  í´ë°± ì§ˆë¬¸ë“¤
    """
    fallback_questions = [
        "ì¹œêµ¬ê°€ ê°‘ìê¸° 'ìš”ì¦˜ ë„ˆë¬´ ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•„'ë¼ê³  í„¸ì–´ë†“ì•˜ìŠµë‹ˆë‹¤. ë‹¹ì‹ ì´ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì–´ë–»ê²Œ ëŒ€ì‘í• ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "íŒ€ í”„ë¡œì íŠ¸ì—ì„œ ì˜ê²¬ì´ ì¶©ëŒí•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‹¹ì‹ ì´ ì–´ë–¤ ì ‘ê·¼ë°©ì‹ìœ¼ë¡œ ì´ ìƒí™©ì„ í•´ê²°í• ì§€ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ì¹œêµ¬ê°€ 'ë‚˜ ì •ë§ ëª»ìƒê²¼ì§€?'ë¼ê³  ì§„ì§€í•˜ê²Œ ë¬¼ì–´ë´…ë‹ˆë‹¤. ë‹¹ì‹ ì´ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ë‹µë³€í• ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ì•½ì†ì— ëŠ¦ì€ ì¹œêµ¬ê°€ ë³€ëª…ì„ ê³„ì†í•©ë‹ˆë‹¤. ë‹¹ì‹ ì´ ì–´ë–¤ ë§ˆìŒìœ¼ë¡œ ì–´ë–»ê²Œ ë°˜ì‘í• ì§€ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "íšŒì˜ì—ì„œ ë‚´ ì˜ê²¬ì´ ë¬´ì‹œë‹¹í•œ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë‹¹ì‹ ì´ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²˜í• ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    ]
    
    import random
    selected = fallback_questions.copy()
    random.shuffle(selected)
    
    # ìš”ì²­ëœ ê°œìˆ˜ë§Œí¼ ë°˜í™˜ (í•„ìš”í•˜ë©´ ì¤‘ë³µ í—ˆìš©)
    if count <= len(selected):
        return selected[:count]
    else:
        result = selected[:]
        while len(result) < count:
            random.shuffle(fallback_questions)
            result.extend(fallback_questions[:count - len(result)])
        return result[:count]

def generate_ai_questions(count: int = 5, difficulty: str = "medium") -> List[str]:
    """
    ë™ê¸° ë˜í¼ í•¨ìˆ˜ - ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€
    """
    return asyncio.run(generate_ai_questions_real(count, difficulty))

def analyze_tf_tendency(text: str) -> float:
    """
    í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ T/F ì„±í–¥ ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ T, 100ì— ê°€ê¹Œìš¸ìˆ˜ë¡ F ì„±í–¥ì…ë‹ˆë‹¤.
    ì‚¬ê³ í˜•(T) ë¬´ì‹¬/ë‹¨ì •/ê°ê´€ì  í‘œí˜„ì´ ê°ì§€ë˜ë©´ ë¬´ì¡°ê±´ Të¡œ ë¶„ë¥˜í•˜ê³ , ê°•ë„ì— ë”°ë¼ ì ìˆ˜ë¥¼ ìë™ ê²°ì •í•©ë‹ˆë‹¤.
    ì‹¸ê°€ì§€ ì—†ëŠ”(ê³µê° ì—†ëŠ” í‰ëª…/ë¬´ì‹¬) ë‹µë³€ì€ Të¡œ ì‚´ì§ ì¹˜ìš°ì¹˜ê²Œ ì ìˆ˜í™”í•©ë‹ˆë‹¤.
    """
    text = text.lower()
    import re

    # ì‚¬ê³ í˜•(T) ê°•í•œ ë¬´ì‹¬/ë‹¨ì •/ê°ê´€ì  í‘œí˜„ íŒ¨í„´ (í™•ì‹¤í•œ ì‚¬ê³ í˜•)
    t_strong_patterns = [
        r"ì–´ì©Œë¼ê³ ", r"ìƒê´€ì—†ì–´", r"ì•Œì•„ì„œ í•´", r"ë‚´ ì•Œ ë°” ì•„ëƒ", r"ê·¸ê²Œ ë‚˜ë‘ ë¬´ìŠ¨ ìƒê´€ì´ì•¼",
        r"ë„¤ ë§ˆìŒëŒ€ë¡œ í•´", r"ë‚´ê°€ ë­˜", r"ê·¸ê±´ ë„¤ ë¬¸ì œì•¼", r"ê·¸ê±´ ì¤‘ìš”í•˜ì§€ ì•Šì•„"
    ]
    t_strong_count = sum(len(re.findall(pattern, text)) for pattern in t_strong_patterns)
    if t_strong_count > 0:
        score = max(15, 30 - (t_strong_count - 1) * 5)
        return float(score)

    # ì‹¸ê°€ì§€ ì—†ëŠ”(ê³µê° ì—†ëŠ” í‰ëª…/ë¬´ì‹¬) íŒ¨í„´ (ì‚´ì§ T)
    t_rude_patterns = [
        r"ëª°ë¼", r"ë”±íˆ", r"ë³„ ìƒê° ì—†ì–´", r"ì‹ ê²½ ì•ˆ ì¨", r"ê´€ì‹¬ ì—†ì–´", r"ê·¸ëƒ¥ ê·¸ë˜", r"ê¸€ì„", r"ìŒ[.\.\,\!\?â€¦]*$", r"ë³„ë¡œì•¼"
    ]
    t_rude_count = sum(len(re.findall(pattern, text)) for pattern in t_rude_patterns)
    if t_rude_count > 0:
        # í‰ëª…/ë¬´ì‹¬ íŒ¨í„´ì´ ê°ì§€ë˜ë©´ 35~45ì (ì‚´ì§ T)
        score = max(35, 45 - (t_rude_count - 1) * 3)
        return float(score)

    # 1. í‚¤ì›Œë“œ(í•µì‹¬/ì•½í•œ) ë° ê°€ì¤‘ì¹˜
    t_keywords_strong = [
        'ë…¼ë¦¬', 'ë¶„ì„', 'íŒë‹¨', 'íš¨ìœ¨', 'ê°ê´€', 'ì‚¬ì‹¤', 'ì¦ê±°', 'í•©ë¦¬', 'ì´ì„±', 'ì²´ê³„',
        'ì •í™•', 'ëª…í™•', 'ì¼ê´€', 'ë°ì´í„°', 'í†µê³„', 'ì¸¡ì •',
        'ë§ë‹¤', 'í‹€ë ¸ë‹¤', 'ì •ë‹µ', 'í™•ì‹¤', 'ëª…ë°±', 'ë¶„ëª…', 'í™•ì¸',
        'ê²€í† ', 'í‰ê°€', 'ê¸°ì¤€', 'ì¡°ê±´', 'í•´ê²°', 'ê°œì„ ',
        'ìµœì ', 'íš¨ê³¼', 'ê²°ì •', 'ì„ íƒ', 'ìš°ì„ ìˆœìœ„', 'ì¤‘ìš”ë„',
        'ë¶ˆê°€ëŠ¥', 'ë¬¸ì œ', 'í•´ë‹µ', 'ë‹µ', 'ë°˜ë“œì‹œ', 'ë¬´ì¡°ê±´', 'ì²´í¬', 'ì‹¤ìš©ì ', 'ê³„ì‚°'
    ]
    t_keywords_weak = [
        'ê³„íš', 'ì „ëµ', 'ëª©í‘œ', 'ì„±ê³¼', 'ë°©ë²•', 'í•´ì•¼', 'í•´ì•¼ì§€', 'í•˜ì', 'ëë‹¤', 'ì•ˆ ë¼', 'ì•ˆ ë¨',
        'í™•ì‹¤íˆ', 'ë¶„ëª…íˆ', 'ì •í™•íˆ', 'ë‹¹ì—°íˆ', 'ë°”ë¡œ', 'ë¨¼ì €', 'ìš°ì„ ', 'ì¼ë‹¨', 'ì •ë¦¬', 'íš¨ê³¼ì ', 'íš¨ìœ¨ì ',
        'ê°„ë‹¨', 'ë³µì¡', 'ê°€ëŠ¥', 'ëë‹¤', 'ìš°ì„ ', 'ì¼ë‹¨', 'í¸í•´', 'í¸ë¦¬', 'ì‰½ë‹¤', 'ì–´ë µë‹¤', 'ì‹œê°„', 'ê°€ê²©', 'ë¹„ìš©'
    ]
    f_keywords_strong = [
        'ê°ì •', 'ë§ˆìŒ', 'ê³µê°', 'ë°°ë ¤', 'ì´í•´', 'ì¡°í™”', 'í˜‘ë ¥', 'ê´€ê³„', 'ì†Œí†µ', 'ì¹œë°€',
        'ê°€ì¹˜', 'ì˜ë¯¸', 'ë„ë•', 'ìœ¤ë¦¬', 'ì§€ì›', 'ê²©ë ¤', 'í–‰ë³µ', 'ìŠ¬í”„', 'ê±±ì •', 'ë¯¸ì•ˆ', 'ê³ ë§ˆ', 'ì†Œì¤‘', 'ì‚¬ë‘',
        'ë”°ëœ»', 'í¬ê·¼', 'ì•„ëŠ‘', 'í¸ì•ˆ', 'ì•ˆì‹¬', 'ë“ ë“ ', 'ê¸°ë¶„', 'ëŠë‚Œ', 'ë§ˆìŒê°€ì§', 'ì‹¬ì •',
        'í•¨ê»˜', 'ê°™ì´', 'ì„œë¡œ', 'ìš°ë¦¬ ëª¨ë‘', 'ì¹œêµ¬', 'ê°€ì¡±', 'ì‚¬ëŒë“¤', 'ë™ë£Œë“¤',
        'ì˜ˆë»', 'ê·€ì—¬ì›Œ', 'ì°©í•´', 'ë©‹ì ¸', 'ì¢‹ì•„í•´', 'ì‹«ì–´í•´'
    ]
    f_keywords_weak = [
        'ê¸°ë»', 'ì¦ê±°ì›Œ', 'ì‹ ë‚˜', 'í–‰ë³µí•´', 'ë§Œì¡±', 'ë¿Œë“¯', 'ì†ìƒ', 'ì§œì¦', 'í™”ë‚˜', 'ë‹µë‹µ', 'ë¶ˆì•ˆ', 'ì‹ ê²½ ì“°ì—¬',
        'ìš°ë¦¬', 'ë‹¤í•¨ê»˜', 'í•¨ê»˜ í•˜ì', 'ê°™ì´ í•˜ì', 'ë§ˆìŒì—', 'ë”°ëœ»', 'í¬ê·¼', 'ë³´ê³  ì‹¶ì–´', 'ë§Œë‚˜ê³  ì‹¶ì–´', 'í•˜ê³  ì‹¶ì–´'
    ]

    # ê°€ì¤‘ì¹˜ ì ìš© ì¹´ìš´íŠ¸
    t_count = sum(2 for keyword in t_keywords_strong if keyword in text) + sum(1 for keyword in t_keywords_weak if keyword in text)
    f_count = sum(2 for keyword in f_keywords_strong if keyword in text) + sum(1 for keyword in f_keywords_weak if keyword in text)

    # íŒ¨í„´/ì–´ì¡°/êµ¬ì¡° ë¶„ì„(ê¸°ì¡´ê³¼ ë™ì¼)
    f_patterns = [
        r'ì–´ë–»ê²Œ ìƒê°|ì–´ë–¤ ëŠë‚Œ|ê´œì°®ì„ê¹Œ|ì–´ë–¨ê¹Œ|ì¢‹ì„ ê²ƒ ê°™|ë‚˜ì  ê²ƒ ê°™',
        r'í•˜ë©´ ì¢‹ê² |í–ˆìœ¼ë©´|ì¸ ê²ƒ ê°™|ëŠë‚Œì´|ê¸°ë¶„ì´',
        r'í•¨ê»˜|ê°™ì´|ì„œë¡œ|ìš°ë¦¬|ëª¨ë‘|ë‹¤í•¨ê»˜',
        r'ë¯¸ì•ˆ|ê³ ë§ˆì›Œ|ì‚¬ë‘|ì†Œì¤‘|ì•„ê»´|ì±™ê¸°',
        r'ê³µê°|ì´í•´|ìœ„ë¡œ|ê²©ë ¤|ì‘ì›',
        r'ì¢‹ì•„|ì‹«ì–´|ì˜ˆë»|ê·€ì—¬ì›Œ|ì¬ë°Œ|ì§€ë£¨',
        r'ê¸°ë¶„ ì¢‹|ëŠë‚Œ ì¢‹|ë§ˆìŒì—|ë”°ëœ»|í¬ê·¼',
        r'í•˜ê³  ì‹¶ì–´|ê°€ê³  ì‹¶ì–´|ë³´ê³  ì‹¶ì–´|ë§Œë‚˜ê³  ì‹¶ì–´',
        r'ê°™ì´ í•˜ì|í•¨ê»˜ í•˜ì|ìš°ë¦¬ ëª¨ë‘|ë‹¤ ê°™ì´'
    ]
    t_patterns = [
        r'í•´ì•¼ í•œë‹¤|í•´ì•¼ì§€|í•˜ì|í•˜ë©´ ë¼|ë˜ë©´|ì•ˆ ë˜ë©´',
        r'ë‹¹ì—°íˆ|ì •í™•íˆ|ë§ë‹¤|í‹€ë ¸ë‹¤|ì˜³ë‹¤|ê·¸ë¥´ë‹¤|í™•ì‹¤íˆ|ë¶„ëª…íˆ',
        r'íš¨ìœ¨ì |ì²´ê³„ì |ë…¼ë¦¬ì |í•©ë¦¬ì |ê°ê´€ì ',
        r'ì¤‘ìš”í•œ ê±´|í•µì‹¬ì€|ë¬¸ì œëŠ”|í•´ê²°ì±…ì€|ë°©ë²•ì€',
        r'ë¨¼ì €|ìš°ì„ |ì°¨ë¡€ë¡œ|ë‹¨ê³„ë³„ë¡œ|ê³„íšì ìœ¼ë¡œ',
        r'ê·¸ëƒ¥|ë°”ë¡œ|ë¹¨ë¦¬|ì¦‰ì‹œ|ì¼ë‹¨|ìš°ì„ ',
        r'ì•ˆ ë¼|ì•ˆ ë¨|ë˜ë„¤|ëë‹¤|ê°€ëŠ¥|ë¶ˆê°€ëŠ¥',
        r'ì‰½ë‹¤|ì–´ë µë‹¤|ê°„ë‹¨|ë³µì¡|í¸í•´|í¸ë¦¬',
        r'ê³„ì‚°|ë¹„ìš©|ê°€ê²©|ì‹œê°„|íš¨ê³¼|ì‹¤ìš©'
    ]
    f_pattern_count = sum(len(re.findall(pattern, text)) for pattern in f_patterns)
    t_pattern_count = sum(len(re.findall(pattern, text)) for pattern in t_patterns)
    
    soft_tone = len(re.findall(r'ê²ƒ ê°™ì•„|ì¸ ë“¯|ì•„ë§ˆ|í˜¹ì‹œ|ë©´ ì–´ë–¨ê¹Œ|í•˜ë©´ ì¢‹ê² |~ì¸ê°€|~í• ê¹Œ|~ì§€ ì•Šì„ê¹Œ', text))
    firm_tone = len(re.findall(r'ë°˜ë“œì‹œ|ë¬´ì¡°ê±´|í™•ì‹¤íˆ|ë‹¹ì—°íˆ|ëª…ë°±íˆ|ë¶„ëª…íˆ|í•´ì•¼|í•˜ì|ëœë‹¤|ì•ˆ ëœë‹¤', text))
    question_suggestion = len(re.findall(r'\?|í• ê¹Œ|ì–´ë–¨ê¹Œ|ì¢‹ì„ê¹Œ|ì–´ë•Œ|ê´œì°®ì„ê¹Œ', text))
    statement_command = len(re.findall(r'ë‹¤\.|ì´ë‹¤\.|í•˜ì\.|í•´ì•¼\.|ëœë‹¤\.|ì•ˆ ëœë‹¤\.', text))
    
    total_keywords = t_count + f_count
    total_patterns = f_pattern_count + t_pattern_count  
    total_tone = soft_tone + firm_tone
    total_structure = question_suggestion + statement_command
    
    # 2. í‚¤ì›Œë“œ ì ìˆ˜(ê°€ì¤‘ì¹˜ ë°˜ì˜)
    if total_keywords == 0:
        keyword_score = 50
    else:
        t_ratio = t_count / total_keywords if total_keywords > 0 else 0
        f_ratio = f_count / total_keywords if total_keywords > 0 else 0
        if t_ratio > f_ratio:
            intensity = min(t_count, 4)
            keyword_score = 25 - (intensity * 7)  # ê¸°ì¡´ 30ì—ì„œ 25ë¡œ, ê°•ë„ë³„ -7ì ì”©
        elif f_ratio > t_ratio:
            intensity = min(f_count, 4)
            keyword_score = 75 + (intensity * 7)  # ê¸°ì¡´ 70ì—ì„œ 75ë¡œ, ê°•ë„ë³„ +7ì ì”©
        else:
            keyword_score = 50
    
    # 3. íŒ¨í„´/ì–´ì¡°/êµ¬ì¡° ì ìˆ˜(ë™ì /ì• ë§¤ì‹œ ê°€ì¤‘ì¹˜ ì¦ê°€)
    if total_patterns == 0:
        pattern_score = 50
    else:
        t_pattern_ratio = t_pattern_count / total_patterns
        f_pattern_ratio = f_pattern_count / total_patterns
        if t_pattern_ratio > f_pattern_ratio:
            intensity = min(t_pattern_count, 3)
            pattern_score = 30 - (intensity * 5)
        elif f_pattern_ratio > t_pattern_ratio:
            intensity = min(f_pattern_count, 3)
            pattern_score = 70 + (intensity * 5)
        else:
            pattern_score = 50
    
    if total_tone == 0:
        tone_score = 50
    else:
        if firm_tone > soft_tone:
            intensity = min(firm_tone, 2)
            tone_score = 25 - (intensity * 5)
        elif soft_tone > firm_tone:
            intensity = min(soft_tone, 2)
            tone_score = 75 + (intensity * 5)
        else:
            tone_score = 50
    
    if total_structure == 0:
        structure_score = 50
    else:
        if statement_command > question_suggestion:
            structure_score = 30
        elif question_suggestion > statement_command:
            structure_score = 70
        else:
            structure_score = 50
    
    text_length = len(text.replace(' ', ''))
    # ë™ì /ì• ë§¤í• ìˆ˜ë¡ íŒ¨í„´/ì–´ì¡°/êµ¬ì¡° ê°€ì¤‘ì¹˜ ì¦ê°€
    if total_keywords == 0 or abs(t_count - f_count) <= 2:
        keyword_weight = 0.25
        pattern_weight = 0.3
        tone_weight = 0.25
        structure_weight = 0.2
    elif text_length < 15:
        keyword_weight = 0.5
        pattern_weight = 0.2  
        tone_weight = 0.15
        structure_weight = 0.15
    elif text_length < 30:
        keyword_weight = 0.45
        pattern_weight = 0.25
        tone_weight = 0.15
        structure_weight = 0.15
    elif text_length < 60:
        keyword_weight = 0.4
        pattern_weight = 0.3
        tone_weight = 0.2
        structure_weight = 0.1
    else:
        keyword_weight = 0.35
        pattern_weight = 0.35
        tone_weight = 0.25
        structure_weight = 0.05
    
    final_score = (keyword_score * keyword_weight + 
                   pattern_score * pattern_weight + 
                   tone_score * tone_weight +
                   structure_score * structure_weight)
    
    # ê°•í•œ í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤(ê¸°ì¡´ ìœ ì§€)
    strong_t_words = ['ë‹¹ì—°', 'í™•ì‹¤', 'ë§ë‹¤', 'í‹€ë ¸', 'í•´ì•¼', 'ëª…ë°±', 'ë¶„ëª…', 'í™•ì‹¤íˆ']
    strong_f_words = ['ì‚¬ë‘', 'ì†Œì¤‘', 'ë°°ë ¤', 'ê³µê°', 'ë§ˆìŒ', 'ê°ì •']
    strong_t = sum(1 for word in strong_t_words if word in text)
    strong_f = sum(1 for word in strong_f_words if word in text)
    if strong_t > strong_f and strong_t > 0:
        bonus = min(strong_t * 3, 8)
        final_score = max(final_score - bonus, 20)
    elif strong_f > strong_t and strong_f > 0:
        bonus = min(strong_f * 3, 8)
        final_score = min(final_score + bonus, 80)
    
    return min(max(final_score, 15), 85)

def generate_f_friendly_response(question: str, answer: str, score: float) -> str:
    """
    F ì„±í–¥ ìƒëŒ€ì—ê²Œ ë” íš¨ê³¼ì ì¸ ë‹µë³€ì„ ì œì•ˆí•©ë‹ˆë‹¤.
    í•œ ì¤„ ì‹¤ì²œ íŒ(ì˜ˆ: 'ì¢€ ë” ì¹œì ˆí•˜ê²Œ ëŒ€í•˜ì„¸ìš”!')ì„ ë§¨ ìœ„ì—, ê·¸ ì•„ë˜ ëŒ€ì•ˆ ë‹µë³€ì„ ë…¸ì¶œí•©ë‹ˆë‹¤.
    """
    import random
    # ì‹¤ì²œ íŒ pool (êµ¬ì–´ì²´+í•˜ì´ë¼ì´íŠ¸ ì ìš©)
    def highlight_tip(tip):
        return f"<span style='font-size:1.2em;color:#ff6600;font-weight:bold'>{tip}</span>"
    f_tips_strong = [
        highlight_tip("ìƒëŒ€ë°©ì˜ ì…ì¥ì—ì„œ í•œ ë²ˆ ë” ìƒê°í•´ë³´ë©´ ì–´ë–¨ê¹Œìš”?"),
        highlight_tip("ìƒëŒ€ê°€ í˜ë“¤ì–´í•  ë•Œ ë¨¼ì € ê³µê°ì˜ ë§ì„ ê±´ë„¤ë³´ì„¸ìš”."),
        highlight_tip("ìƒëŒ€ì˜ ê°ì •ì„ ë¨¼ì € ì¸ì •í•´ì£¼ëŠ” í•œë§ˆë””ê°€ í° í˜ì´ ë©ë‹ˆë‹¤."),
        highlight_tip("ìƒëŒ€ê°€ ë‚´ ë§ì„ ë“£ê³  ì–´ë–¤ ê¸°ë¶„ì¼ì§€ ìƒìƒí•´ë³´ì„¸ìš”."),
        highlight_tip("ìƒëŒ€ë°©ì´ ìœ„ë¡œë°›ì„ ìˆ˜ ìˆë„ë¡ ë”°ëœ»í•˜ê²Œ í‘œí˜„í•´ë³´ì„¸ìš”."),
        highlight_tip("ìƒëŒ€ì˜ ë§ˆìŒì„ í—¤ì•„ë¦¬ëŠ” ë§ í•œë§ˆë””ê°€ í•„ìš”í•  ë•Œì…ë‹ˆë‹¤."),
        highlight_tip("ìƒëŒ€ê°€ ì›í•˜ëŠ” ê²Œ ë­”ì§€ ì§ì ‘ ë¬¼ì–´ë³´ëŠ” ê²ƒë„ ì¢‹ì•„ìš”."),
        highlight_tip("ìƒëŒ€ê°€ ì¡°ì–¸ì„ ì›í•˜ì§€ ì•Šì„ ë•ŒëŠ”, ê·¸ëƒ¥ ë“¤ì–´ì£¼ëŠ” ê²ƒë§Œìœ¼ë¡œë„ ì¶©ë¶„í•´ìš”."),
        highlight_tip("ìƒëŒ€ê°€ ì‹¤ìˆ˜í–ˆì„ ë•ŒëŠ” 'ê´œì°®ì•„, ëˆ„êµ¬ë‚˜ ê·¸ëŸ´ ìˆ˜ ìˆì–´'ë¼ê³  ë§í•´ë³´ì„¸ìš”."),
        highlight_tip("ìƒëŒ€ê°€ ê¸°ë»í•  ë•ŒëŠ” í•¨ê»˜ ê¸°ë»í•´ ì£¼ì„¸ìš”."),
        highlight_tip("ìƒëŒ€ê°€ ì¡°ìš©í•  ë•ŒëŠ” ì–µì§€ë¡œ ë§ì‹œí‚¤ì§€ ë§ê³  ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”."),
        highlight_tip("ìƒëŒ€ê°€ ê³ ë¯¼ì„ í„¸ì–´ë†“ìœ¼ë©´, 'ë„¤ê°€ ê·¸ë ‡ê²Œ ëŠë¼ëŠ” ê²Œ ì´í•´ë¼'ë¼ê³  ê³µê°í•´ ì£¼ì„¸ìš”."),
        highlight_tip("ìƒëŒ€ê°€ í™”ë‚¬ì„ ë•ŒëŠ” ë°”ë¡œ ì¡°ì–¸í•˜ì§€ ë§ê³ , ê°ì •ì„ ë¨¼ì € ë°›ì•„ì£¼ì„¸ìš”."),
        highlight_tip("ìƒëŒ€ê°€ ìŠ¬í¼í•  ë•ŒëŠ” 'í˜ë“¤ì—ˆê² ë‹¤' í•œë§ˆë””ê°€ í° í˜ì´ ë©ë‹ˆë‹¤."),
        highlight_tip("ìƒëŒ€ê°€ ë¶ˆì•ˆí•´í•  ë•ŒëŠ” 'ë„¤ê°€ ê±±ì •í•˜ëŠ” ê²Œ ë­”ì§€ ë§í•´ì¤„ë˜?'ë¼ê³  ë¬¼ì–´ë³´ì„¸ìš”."),
        highlight_tip("ìƒëŒ€ê°€ ê¸°ë¶„ì´ ì¢‹ì•„ ë³´ì´ë©´, 'ì˜¤ëŠ˜ í‘œì •ì´ ë°ì•„ ë³´ì—¬ì„œ ë‚˜ë„ ê¸°ë¶„ì´ ì¢‹ì•„'ë¼ê³  ë§í•´ë³´ì„¸ìš”."),
        highlight_tip("ìƒëŒ€ê°€ í˜ë“¤ì–´í•  ë•ŒëŠ” 'ë‚´ê°€ ì˜†ì— ìˆì–´ì¤„ê²Œ'ë¼ê³  ë§í•´ë³´ì„¸ìš”."),
        highlight_tip("ìƒëŒ€ê°€ ì¡°ìš©íˆ ìˆê³  ì‹¶ì–´í•  ë•ŒëŠ”, ë°°ë ¤í•´ ì£¼ì„¸ìš”."),
        highlight_tip("ìƒëŒ€ê°€ ê°ì •ì„ í‘œí˜„í•  ë•ŒëŠ”, 'ë„¤ê°€ ê·¸ë ‡ê²Œ ëŠë¼ëŠ” ê±° ì •ë§ ì¤‘ìš”í•´'ë¼ê³  ë§í•´ë³´ì„¸ìš”."),
        highlight_tip("ìƒëŒ€ê°€ ê³ ë¯¼ì„ ë§í•  ë•ŒëŠ”, 'ë„¤ê°€ ë§í•´ì¤˜ì„œ ê³ ë§ˆì›Œ'ë¼ê³  í•´ë³´ì„¸ìš”."),
        highlight_tip("ìƒëŒ€ê°€ ì‹¤ìˆ˜í•´ë„ ê´œì°®ì•„, ë‚˜ë„ ê·¸ëŸ° ì  ìˆì–´'ë¼ê³  ê³µê°í•´ ì£¼ì„¸ìš”.")
    ]
    f_tips_neutral = [
        highlight_tip("ì¡°ê¸ˆ ë” ë¶€ë“œëŸ½ê²Œ í‘œí˜„í•´ë³´ë©´ ìƒëŒ€ê°€ ë” í¸ì•ˆí•´í•  ê±°ì˜ˆìš”."),
        highlight_tip("ìƒëŒ€ì˜ ê°ì •ë„ í•¨ê»˜ ê³ ë ¤í•´ë³´ë©´ ì¢‹ê² ì–´ìš”."),
        highlight_tip("ì¢€ ê³µê° ì¢€ í•´ì¤˜!"),
        highlight_tip("ìƒëŒ€ë°©ì˜ ê¸°ë¶„ì„ í•œ ë²ˆ ë” ìƒê°í•´ë³´ë©´ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”.")
    ]
    f_tips_weak = [
        highlight_tip("ì´ë¯¸ ì¶©ë¶„íˆ ê³µê°í•˜ê³  ê³„ì„¸ìš”!"),
        highlight_tip("ì§€ê¸ˆì²˜ëŸ¼ë§Œ í•´ë„ ì¢‹ì•„ìš”!"),
        highlight_tip("ìƒëŒ€ë°©ê³¼ ê°ì •ì„ ë‚˜ëˆ„ëŠ” íƒœë„ê°€ ë©‹ì ¸ìš”!")
    ]
    # ì ìˆ˜ì— ë”°ë¼ íŒ ì„ íƒ
    if score < 30:
        tip = random.choice(f_tips_strong)
    elif score < 60:
        tip = random.choice(f_tips_neutral)
    else:
        tip = random.choice(f_tips_weak)

    # ëŒ€ì•ˆ ë‹µë³€ pool (50ê°œ, ë‹¤ì–‘í•œ ìƒí™©/íŒ¨í„´/ê³µê°/ê²©ë ¤/ê²½ì²­/ì¸ì •/ê¸°ì¨ ë“±)
    alternative_pool = [
    f"{answer} ëŒ€ì‹ , 'ë„¤ê°€ ê·¸ë ‡ê²Œ ëŠë¼ëŠ” ê²Œ ì •ë§ ì´í•´ë¼. í˜¹ì‹œ ë‚´ê°€ ë„ìš¸ ìˆ˜ ìˆëŠ” ê²Œ ìˆì„ê¹Œ?'ë¼ê³  ë§í•´ë³´ì„¸ìš”.",
    f"{answer}ì— ê³µê°ì˜ í•œë§ˆë””ë¥¼ ë”í•´ë³´ë©´ ìƒëŒ€ê°€ ë” í¸ì•ˆí•´í•  ìˆ˜ ìˆì–´ìš”.",
    f"{answer}ë„ ì¢‹ì§€ë§Œ, 'ë„¤ê°€ í˜ë“¤ì—ˆê² ë‹¤. ë‚´ ì–˜ê¸°ë„ ë“¤ì–´ì¤„ë˜?'ë¼ê³  ëŒ€í™”ë¥¼ ì´ì–´ê°€ë©´ ë” ì¢‹ì„ ê²ƒ ê°™ì•„ìš”.",
    f"{answer}ì— 'ë‚´ê°€ ë„¤ ì…ì¥ì´ì—ˆì–´ë„ ë¹„ìŠ·í•˜ê²Œ ëŠê¼ˆì„ ê²ƒ ê°™ì•„'ë¼ëŠ” ë§ì„ ë§ë¶™ì—¬ë³´ì„¸ìš”.",
    f"{answer}ì— 'ë„¤ê°€ ì´ë ‡ê²Œ ì†”ì§í•˜ê²Œ ë§í•´ì¤˜ì„œ ê³ ë§ˆì›Œ'ë¼ê³  í•´ë³´ì„¸ìš”.",
    f"{answer}ì— 'ë„¤ê°€ ì›í•˜ëŠ” ê²Œ ìˆìœ¼ë©´ ì–¸ì œë“  ë§í•´ì¤˜'ë¼ê³  ë§ˆë¬´ë¦¬í•´ë³´ì„¸ìš”.",
    f"{answer}ì— 'ë‚´ê°€ ì˜†ì— ìˆì–´ì¤„ê²Œ'ë¼ëŠ” ë§ì„ ì¶”ê°€í•´ë³´ì„¸ìš”.",
    f"{answer}ì— 'ë„¤ê°€ í˜ë“¤ ë•Œ ì–¸ì œë“  ì—°ë½í•´ë„ ë¼'ë¼ê³  í•´ë³´ì„¸ìš”.",
    f"{answer}ì— 'ë„¤ê°€ ê¸°ë»í•˜ëŠ” ëª¨ìŠµì„ ë³´ë‹ˆ ë‚˜ë„ ê¸°ë»'ë¼ê³  ë§í•´ë³´ì„¸ìš”.",
    f"{answer}ì— 'ë„¤ê°€ ì‹¤ìˆ˜í•´ë„ ê´œì°®ì•„, ë‚˜ë„ ê·¸ëŸ° ì  ìˆì–´'ë¼ê³  ê³µê°í•´ë³´ì„¸ìš”.",
    f"{answer}ì— 'ë„¤ê°€ ë‚´ê²Œ ê³ ë¯¼ì„ í„¸ì–´ë†”ì¤˜ì„œ ê³ ë§ˆì›Œ'ë¼ê³  í•´ë³´ì„¸ìš”.",
    f"{answer}ì— 'ë„¤ê°€ ì›í•˜ëŠ” ê²Œ ìˆìœ¼ë©´ ì–¸ì œë“  ë§í•´ì¤˜'ë¼ê³  í•´ë³´ì„¸ìš”.",
    f"{answer}ì— 'ë„¤ê°€ í˜ë“¤ ë•ŒëŠ” ì–¸ì œë“  ë‚´ê²Œ ê¸°ëŒ€ë„ ë¼'ë¼ê³  ë§í•´ë³´ì„¸ìš”.",
    f"{answer}ì— 'ë„¤ê°€ ë‚´ê²Œ ì†”ì§í•˜ê²Œ ë§í•´ì¤˜ì„œ ê³ ë§ˆì›Œ'ë¼ê³  í•´ë³´ì„¸ìš”.",
    f"{answer}ì— 'ë„¤ê°€ í˜ë“¤ ë•ŒëŠ” ë‚´ê°€ ê³ì— ìˆì–´ì¤„ê²Œ'ë¼ê³  í•´ë³´ì„¸ìš”.",
    f"{answer}ì— 'ë„¤ê°€ ê¸°ë»í•  ë•ŒëŠ” í•¨ê»˜ ê¸°ë»í•´ì¤„ê²Œ'ë¼ê³  ë§í•´ë³´ì„¸ìš”.",
    f"{answer}ì— 'ë„¤ê°€ ìŠ¬í”Œ ë•ŒëŠ” ê³ì— ìˆì–´ì¤„ê²Œ'ë¼ê³  í•´ë³´ì„¸ìš”.",
    f"{answer}ì— 'ë„¤ê°€ í™”ë‚  ë•ŒëŠ” ê°ì •ì„ ë¨¼ì € ë°›ì•„ì¤„ê²Œ'ë¼ê³  í•´ë³´ì„¸ìš”.",
    f"{answer}ì— 'ë„¤ê°€ ë¶ˆì•ˆí•  ë•ŒëŠ” ë„¤ ê±±ì •ì„ ë“¤ì–´ì¤„ê²Œ'ë¼ê³  í•´ë³´ì„¸ìš”.",
    f"{answer}ì— 'ë„¤ê°€ ì¡°ìš©íˆ ìˆê³  ì‹¶ì„ ë•ŒëŠ” ê¸°ë‹¤ë ¤ì¤„ê²Œ'ë¼ê³  í•´ë³´ì„¸ìš”.",
    f"{answer}ì— 'ë„¤ê°€ ê°ì •ì„ í‘œí˜„í•  ë•ŒëŠ” ì†Œì¤‘í•˜ê²Œ ë“¤ì–´ì¤„ê²Œ'ë¼ê³  í•´ë³´ì„¸ìš”.",
    f"{answer}ì— 'ë„¤ê°€ ê³ ë¯¼ì„ ë§í•´ì¤˜ì„œ ê³ ë§ˆì›Œ'ë¼ê³  í•´ë³´ì„¸ìš”."
    ]
    # ëŒ€ì•ˆ ë‹µë³€ í…œí”Œë¦¿ pool (20ê°œ ì´ìƒ, answer í™œìš©/ë¹„í™œìš© í˜¼í•©, ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°)
    alternative_templates = [
    "ìƒëŒ€ì˜ ê°ì •ì„ ë¨¼ì € ì¸ì •í•´ì£¼ëŠ” í•œë§ˆë””ê°€ í° í˜ì´ ë©ë‹ˆë‹¤.",
    "ìƒëŒ€ë°©ì´ í˜ë“¤ì–´í•  ë•ŒëŠ” ë¨¼ì € ê³µê°ì˜ ë§ì„ ê±´ë„¤ë³´ì„¸ìš”.",
    "ìƒëŒ€ì˜ ì…ì¥ì„ ë¨¼ì € ë¬¼ì–´ë´ ì£¼ì„¸ìš”. 'ë„¤ ì…ì¥ì—ì„  ì–´ë• ì–´?'ë¼ê³ ìš”.",
    "ìƒëŒ€ê°€ ë§í•  ë•ŒëŠ” ë¼ì–´ë“¤ì§€ ë§ê³  ëê¹Œì§€ ë“¤ì–´ì£¼ì„¸ìš”.",
    "ìƒëŒ€ê°€ ìš¸ê±°ë‚˜ í™”ë‚  ë•ŒëŠ” ì¡°ìš©íˆ ê³ì— ìˆì–´ì£¼ëŠ” ê²ƒë„ í° í˜ì´ ë©ë‹ˆë‹¤.",
    "ìƒëŒ€ì˜ ê°ì •ì„ ë¶€ì •í•˜ì§€ ë§ê³ , 'ê·¸ëŸ´ ìˆ˜ë„ ìˆì§€'ë¼ê³  ì¸ì •í•´ ì£¼ì„¸ìš”.",
    "ìƒëŒ€ê°€ ì›í•˜ëŠ” ê²Œ ë­”ì§€ ì§ì ‘ ë¬¼ì–´ë³´ëŠ” ê²ƒë„ ì¢‹ì•„ìš”.",
    "ìƒëŒ€ê°€ ì¡°ì–¸ì„ ì›í•˜ì§€ ì•Šì„ ë•ŒëŠ”, ê·¸ëƒ¥ ë“¤ì–´ì£¼ëŠ” ê²ƒë§Œìœ¼ë¡œë„ ì¶©ë¶„í•´ìš”.",
    "ìƒëŒ€ê°€ ì‹¤ìˆ˜í–ˆì„ ë•ŒëŠ” 'ê´œì°®ì•„, ëˆ„êµ¬ë‚˜ ê·¸ëŸ´ ìˆ˜ ìˆì–´'ë¼ê³  ë§í•´ë³´ì„¸ìš”.",
    "ìƒëŒ€ê°€ ê¸°ë»í•  ë•ŒëŠ” í•¨ê»˜ ê¸°ë»í•´ ì£¼ì„¸ìš”.",
    "ìƒëŒ€ê°€ ì¡°ìš©í•  ë•ŒëŠ” ì–µì§€ë¡œ ë§ì‹œí‚¤ì§€ ë§ê³  ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”.",
    "ìƒëŒ€ê°€ ê³ ë¯¼ì„ í„¸ì–´ë†“ìœ¼ë©´, 'ë„¤ê°€ ê·¸ë ‡ê²Œ ëŠë¼ëŠ” ê²Œ ì´í•´ë¼'ë¼ê³  ê³µê°í•´ ì£¼ì„¸ìš”.",
    "ìƒëŒ€ê°€ í™”ë‚¬ì„ ë•ŒëŠ” ë°”ë¡œ ì¡°ì–¸í•˜ì§€ ë§ê³ , ê°ì •ì„ ë¨¼ì € ë°›ì•„ì£¼ì„¸ìš”.",
    "ìƒëŒ€ê°€ ìŠ¬í¼í•  ë•ŒëŠ” 'í˜ë“¤ì—ˆê² ë‹¤' í•œë§ˆë””ê°€ í° í˜ì´ ë©ë‹ˆë‹¤.",
    "ìƒëŒ€ê°€ ë¶ˆì•ˆí•´í•  ë•ŒëŠ” 'ë„¤ê°€ ê±±ì •í•˜ëŠ” ê²Œ ë­”ì§€ ë§í•´ì¤„ë˜?'ë¼ê³  ë¬¼ì–´ë³´ì„¸ìš”.",
    "ìƒëŒ€ê°€ ê¸°ë¶„ì´ ì¢‹ì•„ ë³´ì´ë©´, 'ì˜¤ëŠ˜ í‘œì •ì´ ë°ì•„ ë³´ì—¬ì„œ ë‚˜ë„ ê¸°ë¶„ì´ ì¢‹ì•„'ë¼ê³  ë§í•´ë³´ì„¸ìš”.",
    "ìƒëŒ€ê°€ í˜ë“¤ì–´í•  ë•ŒëŠ” 'ë‚´ê°€ ì˜†ì— ìˆì–´ì¤„ê²Œ'ë¼ê³  ë§í•´ë³´ì„¸ìš”.",
    "ìƒëŒ€ê°€ ì¡°ìš©íˆ ìˆê³  ì‹¶ì–´í•  ë•ŒëŠ”, ë°°ë ¤í•´ ì£¼ì„¸ìš”.",
    "ìƒëŒ€ê°€ ê°ì •ì„ í‘œí˜„í•  ë•ŒëŠ”, 'ë„¤ê°€ ê·¸ë ‡ê²Œ ëŠë¼ëŠ” ê±° ì •ë§ ì¤‘ìš”í•´'ë¼ê³  ë§í•´ë³´ì„¸ìš”.",
    "ìƒëŒ€ê°€ ê³ ë¯¼ì„ ë§í•  ë•ŒëŠ”, 'ë„¤ê°€ ë§í•´ì¤˜ì„œ ê³ ë§ˆì›Œ'ë¼ê³  í•´ë³´ì„¸ìš”.",
    "ìƒëŒ€ê°€ ì‹¤ìˆ˜í•´ë„ ê´œì°®ì•„, ë‚˜ë„ ê·¸ëŸ° ì  ìˆì–´'ë¼ê³  ê³µê°í•´ ì£¼ì„¸ìš”.",
    "ìƒëŒ€ê°€ í˜ë“¤ ë•ŒëŠ” ì–¸ì œë“  ë‚´ê²Œ ê¸°ëŒ€ë„ ë¼'ë¼ê³  ë§í•´ë³´ì„¸ìš”.",
    "ìƒëŒ€ê°€ ë‚´ê²Œ ì†”ì§í•˜ê²Œ ë§í•´ì¤˜ì„œ ê³ ë§ˆì›Œ'ë¼ê³  í•´ë³´ì„¸ìš”.",
    "ìƒëŒ€ê°€ í˜ë“¤ ë•ŒëŠ” ë‚´ê°€ ê³ì— ìˆì–´ì¤„ê²Œ'ë¼ê³  í•´ë³´ì„¸ìš”.",
    "ìƒëŒ€ê°€ ê¸°ë»í•  ë•ŒëŠ” í•¨ê»˜ ê¸°ë»í•´ì¤„ê²Œ'ë¼ê³  ë§í•´ë³´ì„¸ìš”.",
    "ìƒëŒ€ê°€ ìŠ¬í”Œ ë•ŒëŠ” ê³ì— ìˆì–´ì¤„ê²Œ'ë¼ê³  í•´ë³´ì„¸ìš”.",
    "ìƒëŒ€ê°€ í™”ë‚  ë•ŒëŠ” ê°ì •ì„ ë¨¼ì € ë°›ì•„ì¤„ê²Œ'ë¼ê³  í•´ë³´ì„¸ìš”.",
    "ìƒëŒ€ê°€ ë¶ˆì•ˆí•  ë•ŒëŠ” ë„¤ ê±±ì •ì„ ë“¤ì–´ì¤„ê²Œ'ë¼ê³  í•´ë³´ì„¸ìš”.",
    "ìƒëŒ€ê°€ ì¡°ìš©íˆ ìˆê³  ì‹¶ì„ ë•ŒëŠ” ê¸°ë‹¤ë ¤ì¤„ê²Œ'ë¼ê³  í•´ë³´ì„¸ìš”.",
    "ìƒëŒ€ê°€ ê°ì •ì„ í‘œí˜„í•  ë•ŒëŠ” ì†Œì¤‘í•˜ê²Œ ë“¤ì–´ì¤„ê²Œ'ë¼ê³  í•´ë³´ì„¸ìš”.",
    "ìƒëŒ€ê°€ ê³ ë¯¼ì„ ë§í•´ì¤˜ì„œ ê³ ë§ˆì›Œ'ë¼ê³  í•´ë³´ì„¸ìš”.",
    "ìƒëŒ€ì˜ ì´ì•¼ê¸°ë¥¼ ì¶©ë¶„íˆ ë“¤ì–´ì£¼ê³ , ë‚´ ìƒê°ì€ ë‚˜ì¤‘ì— ì „í•´ë„ ëŠ¦ì§€ ì•Šì•„ìš”.",
    f"ë§Œì•½ '{answer}'ë¼ê³  ë‹µí–ˆë‹¤ë©´, ì´ëŸ° ë§ë„ í•¨ê»˜ í•´ë³´ë©´ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”.",
    f"'{answer}'ë¼ê³ ë§Œ í•˜ê¸°ë³´ë‹¤ëŠ”, ìƒëŒ€ì˜ ê°ì •ì„ ë¨¼ì € ë“¤ì–´ì£¼ëŠ” ê²ƒë„ ì¢‹ì•„ìš”.",
    f"ìƒëŒ€ì˜ ì…ì¥ì„ ë¨¼ì € ë“¤ì–´ì£¼ê³ , ê·¸ ë‹¤ìŒì— '{answer}'ì™€ ê°™ì€ ì˜ê²¬ì„ ì „í•´ë³´ì„¸ìš”.",
    f"'{answer}'ì— 'ë‚´ê°€ ë„¤ ì…ì¥ì´ì—ˆì–´ë„ ë¹„ìŠ·í•˜ê²Œ ëŠê¼ˆì„ ê²ƒ ê°™ì•„'ë¼ëŠ” ë§ì„ ë§ë¶™ì—¬ë³´ì„¸ìš”.",
    f"'{answer}'ì— 'ë„¤ê°€ ì´ë ‡ê²Œ ì†”ì§í•˜ê²Œ ë§í•´ì¤˜ì„œ ê³ ë§ˆì›Œ'ë¼ê³  í•´ë³´ì„¸ìš”.",
    f"'{answer}'ì— 'ë„¤ê°€ ì›í•˜ëŠ” ê²Œ ìˆìœ¼ë©´ ì–¸ì œë“  ë§í•´ì¤˜'ë¼ê³  ë§ˆë¬´ë¦¬í•´ë³´ì„¸ìš”.",
    f"'{answer}'ì— 'ë‚´ê°€ ì˜†ì— ìˆì–´ì¤„ê²Œ'ë¼ëŠ” ë§ì„ ì¶”ê°€í•´ë³´ì„¸ìš”.",
    f"'{answer}'ì— 'ë„¤ê°€ í˜ë“¤ ë•Œ ì–¸ì œë“  ì—°ë½í•´ë„ ë¼'ë¼ê³  í•´ë³´ì„¸ìš”.",
    f"'{answer}'ì— 'ë„¤ê°€ ê¸°ë»í•˜ëŠ” ëª¨ìŠµì„ ë³´ë‹ˆ ë‚˜ë„ ê¸°ë»'ë¼ê³  ë§í•´ë³´ì„¸ìš”.",
    f"'{answer}'ì— 'ë„¤ê°€ ì‹¤ìˆ˜í•´ë„ ê´œì°®ì•„, ë‚˜ë„ ê·¸ëŸ° ì  ìˆì–´'ë¼ê³  ê³µê°í•´ë³´ì„¸ìš”."
    ]
    # ì ìˆ˜ êµ¬ê°„ ì„¸ë¶„í™” (0~100ì„ í…œí”Œë¦¿ ê°œìˆ˜ë§Œí¼ ë‚˜ëˆ”)
    idx = int(min(max(score, 0), 99) * len(alternative_templates) // 100)
    alternative = alternative_templates[idx % len(alternative_templates)]
    return f"âœ… {tip}\n\n{alternative}"

def get_f_friendly_alternatives():
    return [
        "ì•¼ ìš°ë¦¬ ëŒ€í™” ì–´ì©Œë‹¤ ì´ë ‡ê²Œ ì •ì„œì˜ ëìœ¼ë¡œ ê°”ëƒ?",
        "ì§€ê¸ˆ ê°ì •ì„  ë„ˆë¬´ íƒ€ì´íŠ¸í•œë°, ì´ê±° ëŠìœ¼ë©´ ìš°ëƒ?",
        "ìš°ë¦¬ ì§€ê¸ˆ ë§ ì•ˆ í•˜ê³  ìˆì–´ë„ ì„œë¡œ ê°ì • 3ì¤„ ìš”ì•½ ì¤‘ì´ì§€?",
        "ì ê¹ë§Œ, ë‚˜ ë„ˆ ê°ì •ê¹Œì§€ ëŠë¼ê³  ìˆê±°ë“ ? ì•½ê°„ ë²…ì°¨.",
        "ì™€ ë‚˜ ì§€ê¸ˆ ë„ˆ ëŒ€ì‹  ê°ì • ì¨ì£¼ê³  ìˆëŠ” ê¸°ë¶„ì¸ë°?",
        "ìš°ë¦¬ ì§€ê¸ˆ ê³µê°ìœ¼ë¡œ ëŒ€í™”í•˜ëŠ” ê±°ì•¼, ë‹¨ì–´ëŠ” ë¶€ì‚¬ì•¼.",
        "ì´ì œ ìš°ë¦¬ ëŒ€í™”ë„ ê°ì • ë°°í„°ë¦¬ ë‹¬ì•„ì„œ ì¶©ì „ ì¢€ í•´ì•¼ ë  ë“¯.",
        "ì§€ê¸ˆ ë„ˆ ì–˜ê¸°í•˜ëŠ”ë° ë‚´ ë§ˆìŒì´ ë” ë°”ë¹ . ì´ê±° ë§ëƒ?",
        "ê°ì • ë„˜ì¹˜ëŠ” ê±° ì¢‹ì€ë°, ì§€ê¸ˆ ë°© ì•ˆì— ìŠµë„ ëª‡ì´ì•¼?",
        "ì•¼ ê°ì • ë„ˆë¬´ ë‚˜ëˆ´ë‹¤, ì´ì œ ì¡°ê¸ˆë§Œ ì´ê¸°ì ìœ¼ë¡œ ì‚´ì."
    ]

def generate_final_analysis(results: List[Dict]) -> FinalAnalysisResponse:
    """
    ì „ì²´ ì§ˆë¬¸ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ T/F ì„±í–¥ ë¶„ì„ê³¼ F ì„±í–¥ ìƒëŒ€ ëŒ€ì‘ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    if not results:
        return FinalAnalysisResponse(
            overall_tendency="ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
            personality_analysis="",
            communication_strategy="",
            strengths=[],
            growth_areas=[],
            keyword_analysis={}
        )
    
    # ì „ì²´ í‰ê·  ì ìˆ˜ ê³„ì‚°
    total_score = sum(r['score'] for r in results) / len(results)
    
    # ì ìˆ˜ ë¶„í¬ ë¶„ì„
    t_responses = sum(1 for r in results if r['score'] < 40)
    neutral_responses = sum(1 for r in results if 40 <= r['score'] <= 60) 
    f_responses = sum(1 for r in results if r['score'] > 60)
    
    # ì „ì²´ ì„±í–¥ íŒë‹¨
    if total_score < 30:
        overall_tendency = "ê°•í•œ T(ì‚¬ê³ í˜•) ì„±í–¥"
        tendency_desc = "ë…¼ë¦¬ì ì´ê³  ê°ê´€ì ì¸ íŒë‹¨ì„ ì„ í˜¸í•˜ëŠ”"
    elif total_score < 45:
        overall_tendency = "T(ì‚¬ê³ í˜•) ì„±í–¥"
        tendency_desc = "í•©ë¦¬ì  ì‚¬ê³ ë¥¼ ì¤‘ì‹œí•˜ëŠ”"
    elif total_score < 55:
        overall_tendency = "T-F ê· í˜•"
        tendency_desc = "ë…¼ë¦¬ì™€ ê°ì •ì˜ ê· í˜•ì´ ì¡íŒ"
    elif total_score < 70:
        overall_tendency = "F(ê°ì •í˜•) ì„±í–¥"
        tendency_desc = "ê°ì •ê³¼ ê´€ê³„ë¥¼ ì¤‘ì‹œí•˜ëŠ”"
    else:
        overall_tendency = "ê°•í•œ F(ê°ì •í˜•) ì„±í–¥"
        tendency_desc = "ê¹Šì€ ê³µê°ê³¼ ë°°ë ¤ì‹¬ì„ ê°€ì§„"
    
    # ì„±ê²© ë¶„ì„
    consistency = 100 - (max([r['score'] for r in results]) - min([r['score'] for r in results]))
    if consistency > 80:
        consistency_desc = "ì¼ê´€ì„±ì´ ë§¤ìš° ë†’ê³  ì•ˆì •ëœ"
    elif consistency > 60:
        consistency_desc = "ì–´ëŠ ì •ë„ ì¼ê´€ëœ"
    else:
        consistency_desc = "ìƒí™©ì— ë”°ë¼ ìœ ì—°í•˜ê²Œ ëŒ€ì‘í•˜ëŠ”"
    
    personality_analysis = f"""
ë‹¹ì‹ ì€ {tendency_desc} {consistency_desc} ì„±í–¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. 

{len(results)}ê°œì˜ ì§ˆë¬¸ ì¤‘ T ì„±í–¥ ë‹µë³€ì´ {t_responses}ê°œ, ê· í˜•ì  ë‹µë³€ì´ {neutral_responses}ê°œ, F ì„±í–¥ ë‹µë³€ì´ {f_responses}ê°œë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. 
ì „ì²´ì ìœ¼ë¡œ {total_score:.1f}ì ìœ¼ë¡œ {overall_tendency}ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
    """.strip()
    
    # F ì„±í–¥ ìƒëŒ€ ëŒ€ì‘ ì „ëµ
    if total_score < 40:  # T ì„±í–¥ì´ ê°•í•œ ê²½ìš°
        communication_strategy = f"""
F ì„±í–¥ ìƒëŒ€ì™€ì˜ íš¨ê³¼ì ì¸ ì†Œí†µë²•:

ğŸ¯ **í•µì‹¬ ì „ëµ**: ë…¼ë¦¬ì  ì„¤ëª… + ê°ì •ì  ë°°ë ¤

ğŸ“ **ëŒ€í™” ë°©ì‹**:
â€¢ "ê°ê´€ì ìœ¼ë¡œ ë³´ë©´ ì´ë ‡ìŠµë‹ˆë‹¤ë§Œ, ë‹¹ì‹ ì˜ ê¸°ë¶„ì€ ì–´ë– ì‹ ê°€ìš”?"
â€¢ "íš¨ìœ¨ì ì¸ ë°©ë²•ì€ ì´ê²ƒì´ì§€ë§Œ, ëª¨ë‘ê°€ í¸ì•ˆí•œ ë°©ë²•ì„ í•¨ê»˜ ì°¾ì•„ë³´ì£ "
â€¢ "ì‚¬ì‹¤ ë¶„ì„ìƒìœ¼ë¡œëŠ”... í•˜ì§€ë§Œ íŒ€ ë¶„ìœ„ê¸°ë„ ì¤‘ìš”í•˜ë‹ˆê¹Œìš”"

ğŸ’¡ **ì£¼ì˜ì‚¬í•­**:
â€¢ ë„ˆë¬´ ì§ì„¤ì ì¸ í‘œí˜„ë³´ë‹¤ëŠ” ë¶€ë“œëŸ¬ìš´ ì–´ì¡° ì‚¬ìš©
â€¢ ê²°ë¡ ì„ ë¨¼ì € ë§í•˜ê¸°ë³´ë‹¤ ìƒëŒ€ë°© ì˜ê²¬ì„ ë¨¼ì € ë“£ê¸°
â€¢ "ë‹¹ì—°íˆ", "í™•ì‹¤íˆ" ê°™ì€ ë‹¨ì •ì  í‘œí˜„ ìì œ
        """
    elif total_score < 60:  # ê· í˜•ì ì¸ ê²½ìš°
        communication_strategy = f"""
F ì„±í–¥ ìƒëŒ€ì™€ì˜ íš¨ê³¼ì ì¸ ì†Œí†µë²•:

ğŸ¯ **í•µì‹¬ ì „ëµ**: í˜„ì¬ì˜ ê· í˜•ê° í™œìš© + ê°ì •ì  í‘œí˜„ ê°•í™”

ğŸ“ **ëŒ€í™” ë°©ì‹**:
â€¢ "ë…¼ë¦¬ì ìœ¼ë¡œë„ ë§ê³  ê°ì •ì ìœ¼ë¡œë„ ì¢‹ì€ ë°©í–¥ì€..."
â€¢ "í•©ë¦¬ì ì´ë©´ì„œë„ ëª¨ë‘ê°€ ë§Œì¡±í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì°¾ì•„ë³´ì£ "
â€¢ "íš¨ê³¼ì ì´ì§€ë§Œ ë”°ëœ»í•œ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ í•´ë³´ë©´ ì–´ë–¨ê¹Œìš”?"

ğŸ’¡ **ê°•í™” í¬ì¸íŠ¸**:
â€¢ í˜„ì¬ì˜ ê· í˜•ê°ì€ í° ì¥ì  - ì´ë¥¼ ì˜ í™œìš©í•˜ì„¸ìš”
â€¢ F ì„±í–¥ ìƒëŒ€ì—ê²ŒëŠ” ê°ì •ì  í‘œí˜„ì„ ì¡°ê¸ˆ ë” ëŠ˜ë ¤ë³´ì„¸ìš”
â€¢ "ìš°ë¦¬ í•¨ê»˜", "ê°™ì´ ìƒê°í•´ë´ìš”" ê°™ì€ í¬ìš©ì  í‘œí˜„ í™œìš©
        """
    else:  # F ì„±í–¥ì¸ ê²½ìš°
        communication_strategy = f"""
F ì„±í–¥ ìƒëŒ€ì™€ì˜ íš¨ê³¼ì ì¸ ì†Œí†µë²•:

ğŸ¯ **í•µì‹¬ ì „ëµ**: ìì—°ìŠ¤ëŸ¬ìš´ ê³µê°ëŒ€ í˜•ì„±

ğŸ“ **ëŒ€í™” ë°©ì‹**:
â€¢ ì´ë¯¸ F ì„±í–¥ì´ì‹œë¯€ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì˜ ì†Œí†µí•˜ê³  ê³„ì‹­ë‹ˆë‹¤
â€¢ "ë§ˆìŒìœ¼ë¡œ ëŠë¼ê¸°ì—ëŠ”...", "í•¨ê»˜ ìƒê°í•´ë³´ë©´..." ê°™ì€ í‘œí˜„ì´ ìì—°ìŠ¤ëŸ½ê²Œ ë‚˜ì˜¤ì‹¤ ê±°ì˜ˆìš”
â€¢ ê°ì •ì  ê³µê°ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ì†Œí†µì´ í¸ì•ˆí•˜ì‹¤ ê²ƒì…ë‹ˆë‹¤

ğŸ’¡ **ì¶”ê°€ íŒ**:
â€¢ ê°€ë” ê°ê´€ì  ê·¼ê±°ë„ í•¨ê»˜ ì œì‹œí•˜ë©´ ë”ìš± ì„¤ë“ë ¥ ìˆëŠ” ì†Œí†µ ê°€ëŠ¥
â€¢ T ì„±í–¥ì´ ê°•í•œ ìƒëŒ€ë°©ì—ê²ŒëŠ” ë…¼ë¦¬ì  ì„¤ëª…ì„ ë¨¼ì € í•˜ê³  ê°ì •ì  ë°°ë ¤ë¥¼ ë”í•˜ëŠ” ë°©ì‹ ì‹œë„
        """
    
    # ê°•ì  ë¶„ì„
    hashtag_candidates = {
        'T': ['ë…¼ë¦¬', 'ê°ê´€', 'íŒë‹¨', 'íš¨ìœ¨', 'ì¼ê´€'],
        'B': ['ê· í˜•', 'ìœµí†µ', 'ì¡°í™”', 'í†µí•©'],
        'F': ['ê³µê°', 'ë°°ë ¤', 'ê´€ê³„', 'ì´í•´', 'ì†Œí†µ']
    }
    # ì ìˆ˜ êµ¬ê°„ë³„ í•´ì‹œíƒœê·¸ ì„ íƒ
    if total_score < 30:
        strengths = hashtag_candidates['T'][:3]  # ë…¼ë¦¬, ê°ê´€, íŒë‹¨
    elif total_score < 50:
        strengths = hashtag_candidates['T'][:1] + hashtag_candidates['B'][:2]  # ë…¼ë¦¬, ê· í˜•, ìœµí†µ
    else:
        strengths = hashtag_candidates['F'][:3]  # ê³µê°, ë°°ë ¤, ê´€ê³„
    
    # ì„±ì¥ ì˜ì—­
    growth_areas = []
    if total_score < 30:
        growth_areas = [
            "ìƒëŒ€ë°©ì˜ ê°ì •ê³¼ ì…ì¥ ê³ ë ¤í•˜ê¸°",
            "ë¶€ë“œëŸ½ê³  ë”°ëœ»í•œ í‘œí˜„ ë°©ì‹ ì—°ìŠµ",
            "ë…¼ë¦¬ì  ì„¤ëª…ê³¼ ê°ì •ì  ë°°ë ¤ì˜ ì¡°í™”",
            "ìƒëŒ€ë°© ì˜ê²¬ì„ ë¨¼ì € ë“£ëŠ” ìŠµê´€ ê¸°ë¥´ê¸°"
        ]
    elif total_score < 50:
        growth_areas = [
            "F ì„±í–¥ ìƒëŒ€ì™€ ì†Œí†µí•  ë•Œ ê°ì •ì  í‘œí˜„ ëŠ˜ë¦¬ê¸°",
            "ê³µê°ì  ì–¸ì–´ ì‚¬ìš© ì—°ìŠµ",
            "í˜„ì¬ì˜ ê· í˜•ê°ì„ ìƒí™©ì— ë§ê²Œ ì¡°ì ˆí•˜ê¸°",
            "ê°ì •ì  ë‹ˆì¦ˆì— ë” ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•˜ê¸°"
        ]
    else:
        growth_areas = [
            "ê°ì •ì  íŒë‹¨ê³¼ í•¨ê»˜ ê°ê´€ì  ê·¼ê±° ê³ ë ¤í•˜ê¸°",
            "ë•Œë¡œëŠ” ë‹¨í˜¸í•œ ê²°ì •ë„ í•„ìš”í•¨ì„ ì¸ì‹",
            "ë…¼ë¦¬ì  ì„¤ë“ë ¥ ê°•í™”",
            "T ì„±í–¥ ìƒëŒ€ë°©ê³¼ì˜ ì†Œí†µ ë°©ì‹ ë‹¤ì–‘í™”"
        ]
    
    # í‚¤ì›Œë“œ ë¶„ì„ - ê°œì„ ëœ ë²„ì „
    keyword_analysis = {
        'logical_thinking': {},  # ë…¼ë¦¬ì  ì‚¬ê³ 
        'analytical_approach': {},  # ë¶„ì„ì  ì ‘ê·¼
        'emotional_empathy': {},  # ê°ì •ì  ê³µê°
        'relationship_focus': {}  # ê´€ê³„ ì¤‘ì‹¬
    }
    
    # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ì •ì˜ (í™•ì¥ëœ ë²„ì „ - ì§§ì€ ë‹µë³€ì„ ìœ„í•œ ì™„í™”ëœ ê¸°ì¤€)
    keyword_categories = {
        'logical_thinking': [
            # ê¸°ë³¸ í‚¤ì›Œë“œ
            'ë…¼ë¦¬', 'ë¶„ì„', 'íŒë‹¨', 'ì´ì„±', 'í•©ë¦¬', 'ê°ê´€', 'ì²´ê³„', 'ì›ë¦¬', 'ì¼ê´€',
            # ì§§ì€ ë‹µë³€ìš© ì¶”ê°€ í‚¤ì›Œë“œ
            'ë§', 'í‹€', 'í™•ì‹¤', 'ë‹¹ì—°', 'ë¶„ëª…', 'ëª…í™•', 'ì •í™•', 'ê°ê´€ì ', 'ë…¼ë¦¬ì ', 'í•©ë¦¬ì ',
            'ì‚¬ì‹¤', 'ì¦ê±°', 'ê·¼ê±°', 'ì´ìœ ', 'ì›ì¸', 'ê²°ê³¼', 'ë°©ë²•', 'í•´ê²°'
        ],
        'analytical_approach': [
            # ê¸°ë³¸ í‚¤ì›Œë“œ  
            'íš¨ìœ¨', 'ì„±ê³¼', 'ì „ëµ', 'ê³„íš', 'ëª©í‘œ', 'ë°ì´í„°', 'ì¸¡ì •', 'ì •í™•', 'ëª…í™•',
            # ì§§ì€ ë‹µë³€ìš© ì¶”ê°€ í‚¤ì›Œë“œ
            'ê³„íšì ', 'ì²´ê³„ì ', 'ë‹¨ê³„', 'ìˆœì„œ', 'ë¨¼ì €', 'ìš°ì„ ', 'ì¤‘ìš”', 'í•µì‹¬',
            'ë¹„êµ', 'í‰ê°€', 'ê²€í† ', 'í™•ì¸', 'ì„ íƒ', 'ê²°ì •', 'ìµœì ', 'íš¨ê³¼ì '
        ],
        'emotional_empathy': [
            # ê¸°ë³¸ í‚¤ì›Œë“œ
            'ê°ì •', 'ëŠë‚Œ', 'ë§ˆìŒ', 'ê³µê°', 'ì´í•´', 'ìœ„ë¡œ', 'ë”°ëœ»', 'ë°°ë ¤',
            # ì§§ì€ ë‹µë³€ìš© ì¶”ê°€ í‚¤ì›Œë“œ  
            'ì¢‹', 'ì‹«', 'ê¸°ë¶„', 'í–‰ë³µ', 'ìŠ¬', 'í˜ë“¤', 'ê±±ì •', 'ê³ ë¯¼',
            'ë¯¸ì•ˆ', 'ê³ ë§ˆ', 'ì‚¬ë‘', 'ì†Œì¤‘', 'ì˜ˆì˜', 'ê·€ì—¬', 'ì¬ë¯¸', 'ì¦ê±°'
        ],
        'relationship_focus': [
            # ê¸°ë³¸ í‚¤ì›Œë“œ
            'ê´€ê³„', 'ì†Œí†µ', 'í˜‘ë ¥', 'ì¡°í™”', 'ì‚¬ëŒ', 'ì¸ê°„', 'ë„ì›€', 'ì§€ì›', 'ê²©ë ¤',
            # ì§§ì€ ë‹µë³€ìš© ì¶”ê°€ í‚¤ì›Œë“œ
            'í•¨ê»˜', 'ê°™ì´', 'ì„œë¡œ', 'ìš°ë¦¬', 'ì¹œêµ¬', 'ê°€ì¡±', 'ë™ë£Œ', 'íŒ€',
            'ë°°ë ¤', 'ì¡´ì¤‘', 'ì´í•´', 'ë„ì™€', 'ë•', 'ì±™ê¸°', 'ì‘ì›'
        ]
    }
    
    # ê° ë‹µë³€ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì™„í™”ëœ ë§¤ì¹­ ë°©ì‹)
    for r in results:
        answer_text = r['answer'].lower()  # ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ë§¤ì¹­ í–¥ìƒ
        
        for category, keywords in keyword_categories.items():
            for keyword in keywords:
                matches = 0
                
                # 1. ì •í™•í•œ ë‹¨ì–´ ë§¤ì¹­ (ê¸°ì¡´ ë°©ì‹, ë” ë†’ì€ ê°€ì¤‘ì¹˜)
                pattern = r'\b' + re.escape(keyword) + r'\b'
                exact_matches = len(re.findall(pattern, answer_text, re.IGNORECASE))
                matches += exact_matches * 2  # ì •í™• ë§¤ì¹­ì€ 2ë°° ê°€ì¤‘ì¹˜
                
                # 2. ë¶€ë¶„ ë§¤ì¹­ (ì™„í™”ëœ ë°©ì‹, ì§§ì€ í‚¤ì›Œë“œë§Œ)
                if len(keyword) <= 2 and exact_matches == 0:  # 2ê¸€ì ì´í•˜ í‚¤ì›Œë“œë§Œ ë¶€ë¶„ë§¤ì¹­ í—ˆìš©
                    if keyword in answer_text:
                        matches += 1
                
                # 3. ì–´ê·¼ ë§¤ì¹­ (ë™ì‚¬/í˜•ìš©ì‚¬ í™œìš©)
                if exact_matches == 0:
                    # í•œêµ­ì–´ ì–´ê·¼ íŒ¨í„´ë“¤
                    root_patterns = {
                        'ì¢‹': ['ì¢‹ì•„', 'ì¢‹ì€', 'ì¢‹ì„', 'ì¢‹ë‹¤', 'ì¢‹ì§€', 'ì¢‹ë„¤'],
                        'ì‹«': ['ì‹«ì–´', 'ì‹«ì€', 'ì‹«ë‹¤', 'ì‹«ë„¤'],
                        'ë§': ['ë§ì•„', 'ë§ëŠ”', 'ë§ë‹¤', 'ë§ë„¤', 'ë§ì§€'],
                        'í‹€': ['í‹€ë ¤', 'í‹€ë¦°', 'í‹€ë ¸'],
                        'ë„ì™€': ['ë„ì™€ì¤˜', 'ë„ì™€ì£¼', 'ë„ì›€'],
                        'ë•': ['ë„ì™€', 'ë„ì›€'],
                        'í•¨ê»˜': ['ê°™ì´'],
                        'í™•ì‹¤': ['í™•ì‹¤íˆ', 'í™•ì‹¤í•œ']
                    }
                    
                    if keyword in root_patterns:
                        for variant in root_patterns[keyword]:
                            if variant in answer_text:
                                matches += 1
                                break
                    
                    # ì—­ë°©í–¥ ì²´í¬ (í‚¤ì›Œë“œê°€ ë³€í˜•ì–´ì¸ ê²½ìš°)
                    for root, variants in root_patterns.items():
                        if keyword in variants and root in answer_text:
                            matches += 1
                            break
                
                if matches > 0:
                    if keyword not in keyword_analysis[category]:
                        keyword_analysis[category][keyword] = 0
                    keyword_analysis[category][keyword] += matches
    
    return FinalAnalysisResponse(
        overall_tendency=overall_tendency,
        personality_analysis=personality_analysis,
        communication_strategy=communication_strategy,
        strengths=strengths,
        growth_areas=growth_areas,
        keyword_analysis=keyword_analysis
    )

def log_debug(msg):
    with open("debug.log", "a", encoding="utf-8") as f:
        f.write(msg + "\n")

@app.post("/analyze")
async def analyze_text(request: TextRequest):
    with open("debug.log", "a", encoding="utf-8") as f:
        f.write("[DEBUG] analyze_text í•¨ìˆ˜ ì§„ì…!\n")
    log_debug(f"[DEBUG] /analyze ìš”ì²­ ë„ì°©, AI_CLIENT: {AI_CLIENT}")
    try:
        if AI_CLIENT:
            log_debug("[DEBUG] Groq AI ë¶„ì„ ë¶„ê¸° ì§„ì…")
            try:
                prompt = f"""
                ì•„ë˜ ë‹µë³€ì€ T(ì‚¬ê³ í˜•)ì¸ ë‚´ê°€ F(ê°ì •í˜•)ì¸ ìƒëŒ€ì—ê²Œ í•œ ë§ì´ì•¼.
                - F(ê°ì •í˜•) ì„±í–¥ì˜ ìƒëŒ€ê°€ ì´ ë‹µë³€ì„ ë“¤ì—ˆì„ ë•Œ ì–´ë–¤ ëŠë‚Œì¼ì§€, ê·¸ë¦¬ê³  Fì—ê²Œ ë” íš¨ê³¼ì ìœ¼ë¡œ ì†Œí†µí•˜ë ¤ë©´ ì–´ë–»ê²Œ ë°”ê¾¸ë©´ ì¢‹ì„ì§€ ë¶„ì„í•´ì¤˜.
                - ë¶„ì„ ê²°ê³¼(ìì—°ì–´)ì—ëŠ” ë°˜ë“œì‹œ 'ë§¤ìš° ê°•í•œ T ì„±í–¥', 'ê°•í•œ F ì„±í–¥', 'ì•½í•œ T ì„±í–¥', 'Tì™€ Fì˜ ê· í˜•', 'ì¤‘ë¦½', 'ë°¸ëŸ°ìŠ¤' ë“±ê³¼ ê°™ì´ 'ì„±í–¥ì´ OOOí•˜ë‹¤'ë¼ëŠ” ë¬¸êµ¬ë¥¼ ëª…í™•í•˜ê²Œ í¬í•¨í•´ì„œ ì‘ì„±í•´ì¤˜.
                - ë§ˆì§€ë§‰ì—ëŠ” ë°˜ë“œì‹œ "ì ìˆ˜: X" í˜•ì‹ìœ¼ë¡œ 0~100 ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ëª…ì‹œí•´ì¤˜. (0=ë§¤ìš° ê°•í•œ T, 50=ê· í˜•, 100=ë§¤ìš° ê°•í•œ F)
                - ë¶„ì„ ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì¤˜:
                [ë¶„ì„]
                ì„±í–¥ ë¶„ì„ ë° F ì…ì¥ì—ì„œì˜ ë°˜ì‘

                [ê·¼ê±°]
                ë¶„ì„ì˜ ê·¼ê±°

                [ì œì•ˆ]
                1. Fê°€ ê³µê°í•  ìˆ˜ ìˆëŠ” ê°œì„  ì œì•ˆ 1
                2. Fê°€ ê³µê°í•  ìˆ˜ ìˆëŠ” ê°œì„  ì œì•ˆ 2
                3. Fê°€ ê³µê°í•  ìˆ˜ ìˆëŠ” ê°œì„  ì œì•ˆ 3

                [ì‹¤ì²œíŒ]
                F ì„±í–¥ ìƒëŒ€ë¥¼ ìœ„í•œ í•œ ì¤„ ì‹¤ì²œ íŒ

                [ëŒ€ì•ˆ]
                F ì„±í–¥ ìƒëŒ€ë¥¼ ìœ„í•œ ëŒ€ì•ˆ ë‹µë³€

                ì ìˆ˜: X (0=ë§¤ìš° ê°•í•œ T, 50=ê· í˜•, 100=ë§¤ìš° ê°•í•œ F)

                *** ì¤‘ìš”: ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ì˜ì–´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ***

                ë‹µë³€: {request.text.strip()}
                """
                response = await AI_CLIENT.chat.completions.create(
                    messages=[{"role": "user", "content": prompt}],
                    model="llama3-8b-8192",
                )
                result = response.choices[0].message.content
                if result is not None:
                    result = result.strip().upper()
                else:
                    result = ""
                log_debug(f"[Groq AI ì›ë³¸ ì‘ë‹µ]: {result}")
                import re
                # ì ìˆ˜ íŒŒì‹± ì •ê·œì‹ ê°œì„ : ë‹¤ì–‘í•œ ë„ì–´ì“°ê¸°/ì½œë¡ /í•œê¸€ì ì˜¤íƒ€ í—ˆìš©
                score_match = re.search(r"ì \s*ìˆ˜\s*[:ï¼š=\-]?\s*(\d{1,3})", result)
                if score_match:
                    tf_score = float(score_match.group(1))
                elif (not result) or ("429" in result) or ("QUOTA" in result) or ("ERROR" in result):
                    log_debug("[Groq ì‘ë‹µ ë¹„ì •ìƒ, fallbackìœ¼ë¡œ ìì²´ ë¶„ì„ ìˆ˜í–‰]")
                    tf_score = analyze_tf_tendency(request.text)
                    log_debug("[ë¶„ì„ ë¡œì§: fallback]")
                else:
                    if 'T' in result and 'F' not in result:
                        tf_score = 20
                    elif 'F' in result and 'T' not in result:
                        tf_score = 80
                    elif any(k in result for k in ['B', 'ê· í˜•', 'ì¤‘ë¦½', 'ë°¸ëŸ°ìŠ¤']):
                        tf_score = 50
                    elif 'T' in result and 'F' in result:
                        tf_score = 50
                    else:
                        log_debug(f"[Groq AI ì˜ˆì™¸: ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ] {result}")
                        tf_score = analyze_tf_tendency(request.text)
                        log_debug("[ë¶„ì„ ë¡œì§: fallback]")
                    log_debug("[ë¶„ì„ ë¡œì§: groq]")
                # ìƒì„¸ë¶„ì„ íŒŒì‹±
                def extract(tag):
                    content = response.choices[0].message.content
                    if content is None:
                        return ""
                    m = re.search(rf"\[{tag}\](.*?)(?=\[|$)", content, re.DOTALL)
                    return m.group(1).strip() if m else ""
                detailed_analysis = extract("ë¶„ì„")
                reasoning = extract("ê·¼ê±°")
                suggestions_raw = extract("ì œì•ˆ")
                suggestions = [s.strip() for s in suggestions_raw.split("\n") if s.strip()] if suggestions_raw else []
                alternative_response = extract("ëŒ€ì•ˆ")
                tip = extract("ì‹¤ì²œíŒ")
                # ëŒ€ì•ˆë‹µë³€ì´ ì—†ê±°ë‚˜ fallbackì¼ ë•Œ ëœë¤ ë¬¸êµ¬ ì¶”ê°€ (Fìš©, Tê°•/ì•½ êµ¬ë¶„)
                if (not alternative_response or alternative_response.strip() == "Groq ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.") and (not tip or tip.strip() == ""):
                    if tf_score <= 20:
                        one_liner = random.choice(get_t_strong_ment())
                    elif tf_score <= 40:
                        one_liner = random.choice(get_t_mild_ment())
                    else:
                        one_liner = random.choice(get_f_friendly_alternatives())
                    # Groq ëŒ€ì•ˆ ì œì•ˆì´ ìˆìœ¼ë©´ ê·¸ ì•„ë˜ì— ì¶”ê°€
                    groq_tip = tip
                    groq_alt = extract("ëŒ€ì•ˆ")
                    merged = []
                    if groq_tip and groq_tip.strip() != "Groq ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.":
                        merged.append(groq_tip.strip())
                    if groq_alt and groq_alt.strip() != "Groq ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.":
                        merged.append(groq_alt.strip())
                    if merged:
                        alternative_response = one_liner + "\n" + "\n".join(merged)
                    else:
                        alternative_response = one_liner
                else:
                    # ì‹¤ì²œíŒ+ëŒ€ì•ˆì´ ìˆìœ¼ë©´ í•©ì³ì„œ ë°˜í™˜
                    merged = []
                    if tip and tip.strip() != "Groq ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.":
                        merged.append(tip.strip())
                    if alternative_response and alternative_response.strip() != "Groq ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.":
                        merged.append(alternative_response.strip())
                    if merged:
                        alternative_response = "\n".join(merged)
                # --- ìì—°ì–´ ì„±í–¥ íŒŒì‹± ë° ì ìˆ˜ ë³´ì • ---
                def parse_tendency_score(text):
                    text = text.replace(" ", "")
                    # ê°•ë„ ìš°ì„ ìˆœìœ„: ë§¤ìš°ê°•í•œ > ê°•í•œ > ì•½í•œ > ê· í˜•/ì¤‘ë¦½/ë°¸ëŸ°ìŠ¤
                    if re.search(r"ë§¤ìš°ê°•(í•œ)?Tì„±í–¥", text):
                        return 5
                    if re.search(r"ê°•(í•œ)?Tì„±í–¥", text):
                        return 15
                    if re.search(r"ì•½(í•œ)?Tì„±í–¥", text):
                        return 35
                    if re.search(r"Tì™€Fì˜ê· í˜•|ë…¼ë¦¬ì™€ê°ì •ì˜ê· í˜•|ì¤‘ë¦½|ë°¸ëŸ°ìŠ¤", text):
                        return 50
                    if re.search(r"ì•½(í•œ)?Fì„±í–¥", text):
                        return 65
                    if re.search(r"ê°•(í•œ)?Fì„±í–¥", text):
                        return 85
                    if re.search(r"ë§¤ìš°ê°•(í•œ)?Fì„±í–¥", text):
                        return 95
                    if re.search(r"Tì„±í–¥", text):
                        return 40
                    if re.search(r"Fì„±í–¥", text):
                        return 60
                    return None
                # ìì—°ì–´ ì„±í–¥ ì ìˆ˜ ì¶”ì¶œ
                tendency_score = parse_tendency_score(detailed_analysis)
                # ì ìˆ˜ì™€ ìì—°ì–´ê°€ ë¶ˆì¼ì¹˜í•˜ë©´ ìì—°ì–´ ê¸°ì¤€ìœ¼ë¡œ ë³´ì •
                if tendency_score is not None and abs(tf_score - tendency_score) >= 10:
                    log_debug(f"[ì ìˆ˜/ìì—°ì–´ ë¶ˆì¼ì¹˜: Groq ì ìˆ˜={tf_score}, ìì—°ì–´ ì ìˆ˜={tendency_score}, ìì—°ì–´ë¡œ ë³´ì •]")
                    tf_score = tendency_score
                return AnalysisResponse(
                    score=tf_score,
                    detailed_analysis=detailed_analysis,
                    reasoning=reasoning,
                    suggestions=suggestions,
                    alternative_response=alternative_response
                )
            except Exception as e:
                log_debug(f"[Groq AI ì˜ˆì™¸ ë°œìƒ, fallbackìœ¼ë¡œ ìì²´ ë¶„ì„ ìˆ˜í–‰]: {e}")
                tf_score = analyze_tf_tendency(request.text)
                log_debug("[ë¶„ì„ ë¡œì§: fallback]")
                return AnalysisResponse(score=tf_score)
        else:
            log_debug("[DEBUG] Fallback(í‚¤ì›Œë“œ ë¶„ì„) ë¶„ê¸° ì§„ì…")
            tf_score = analyze_tf_tendency(request.text)
            log_debug("[ë¶„ì„ ë¡œì§: fallback]")
            return AnalysisResponse(score=tf_score)
    except Exception as e:
        log_debug(f"[analyze_text ìµœìƒìœ„ ì˜ˆì™¸]: {e}")
        tf_score = analyze_tf_tendency(request.text)
        log_debug("[ë¶„ì„ ë¡œì§: fallback]")
        return AnalysisResponse(score=tf_score)

@app.post("/final_analyze")
async def final_analyze(request: FinalAnalysisRequest):
    try:
        final_result = generate_final_analysis(request.results)
        return final_result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/generate_questions")
async def generate_questions(request: QuestionGenerationRequest):
    """
    AI ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ì§ˆë¬¸ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        questions = await generate_ai_questions_real(count=request.count or 5, difficulty=request.difficulty or "medium")
        return {
            "questions": questions,
            "count": len(questions),
            "difficulty": request.difficulty,
            "generated_by": "AI"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.get("/questions")
async def get_questions(use_ai: str = "false", count: int = 5, difficulty: str = "medium"):
    """
    ì§ˆë¬¸ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤. use_ai=trueì´ë©´ AI ìƒì„± ì§ˆë¬¸ì„, falseì´ë©´ ê¸°ë³¸ ì§ˆë¬¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        use_ai_bool = str(use_ai).lower() in ["true", "1", "yes"]
        if use_ai_bool:
            # AI ìƒì„± ì§ˆë¬¸ ì‚¬ìš©
            questions = await generate_ai_questions_real(count=count, difficulty=difficulty)
            return {
                "questions": questions,
                "source": "AI",
                "count": len(questions),
                "difficulty": difficulty
            }
        else:
            # ê¸°ë³¸ ì§ˆë¬¸ íŒŒì¼ ì‚¬ìš©
            questions_file = Path("question/questions.json")
            if not questions_file.exists():
                # íŒŒì¼ì´ ì—†ìœ¼ë©´ AI ìƒì„± ì§ˆë¬¸ìœ¼ë¡œ ëŒ€ì²´
                questions = await generate_ai_questions_real(count=5, difficulty="medium")
                return {
                    "questions": questions,
                    "source": "AI_fallback",
                    "count": len(questions),
                    "difficulty": "medium"
                }
            
            with open(questions_file, "r", encoding="utf-8") as f:
                questions_data = json.load(f)
            
            questions_data["source"] = "file"
            return questions_data
            
    except json.JSONDecodeError:
        # JSON íŒŒì¼ ì˜¤ë¥˜ ì‹œ AI ìƒì„± ì§ˆë¬¸ìœ¼ë¡œ ëŒ€ì²´
        questions = await generate_ai_questions_real(count=5, difficulty="medium")
        return {
            "questions": questions,
            "source": "AI_fallback",
            "count": len(questions),
            "difficulty": "medium"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/reset_log")
async def reset_log():
    with open("debug.log", "w", encoding="utf-8") as f:
        f.write("[DEBUG] ë¡œê·¸ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!\n")
    return Response(content="ë¡œê·¸ ì´ˆê¸°í™” ì™„ë£Œ", media_type="text/plain")

def get_t_strong_ment():
    return [
        "ë¼ˆ ë§ì•˜ì–´â€¦",
        "ì–´ë””ë³´ì ë°ìŠ¤ë…¸íŠ¸ê°€...",
        "ë³µìˆ˜í•œë‹¤â€¦",
        "ë„Œ ì§„ì§œ ê°ì •ì´ë€ ê²Œ ìˆë‹ˆ?",
        "ë„Œ Dì¡Œë‹¤",
        "ì¡°ë§Œê°„ ìˆœì‚´ë§Œë“¤ì–´ì¤€ë‹¤",
        "ë¡œë´‡ì´ëƒ..?",
        "ìœ  ìŠ¤í‹¸ ë§ˆì´ ë°ìŠ¤ë…¸íŠ¸ ë„˜ë²„ì›~",
        "ìš°ë¦¬ í—¤ì–´ì ¸",
        "ì €ë¦¬ê°€ ã… ã… "
    ]

def get_t_mild_ment():
    return [
        "ê³„ì‚°ê¸°ëƒ?",
        "ë‹˜ ë°°ë ¤ì¢€...",
        "ë¡œë´‡ì´ëƒ?",
        "ì‚´ì‚´í•´ì£¼ì„¸ìš”..",
        "ë‹ˆ ë§ë„ ë§ëŠ”ë°.. ì‚´ì‚´ì¢€ ã… ",
        "ë‚´ ê¸°ë¶„ ì¡´ì¤‘ì¢€ ã… ",
        "ë§ ëŒ€ì‹  ê²°ê³¼?",
        "ê°ì •ë„ ì¢€ ì±™ê¸°ë¼êµ¬!",
        "ë„¤ ë…¼ë¦¬ ë”°ë¼ê°€ë‹¤ ë¨¸ë¦¬ í„°ì ¸ ì£½ê² ì–´",
        "íŒ©íŠ¸ë¶€í„° ì •ë¦¬í•´ë¼? ë‚´ ë§ˆìŒì€ ëˆ„ê°€ ì •ë¦¬í•´ì¤˜?"
    ]

@app.post("/stt")
async def speech_to_text(audio_file: UploadFile = File(...)):
    """
    Speech to text endpoint that accepts audio file uploads
    """
    try:
        if not audio_file:
            raise HTTPException(status_code=400, detail="No audio file provided")
        
        # íŒŒì¼ í™•ì¥ì ê²€ì‚¬
        allowed_extensions = ['.wav', '.mp3', '.ogg', '.m4a']
        filename = audio_file.filename or ""
        file_ext = os.path.splitext(filename)[1].lower()
        if file_ext not in allowed_extensions:
            raise HTTPException(
                status_code=400,
                detail=f"Unsupported file format. Allowed formats: {', '.join(allowed_extensions)}"
            )
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        temp_audio_path = None
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as temp_audio:
                temp_audio_path = temp_audio.name
                content = await audio_file.read()
                temp_audio.write(content)
                temp_audio.flush()
                os.fsync(temp_audio.fileno())
            
            print(f"Processing audio file: {temp_audio_path}")
            # Whisperë¡œ ìŒì„± ì¸ì‹
            result = whisper_model.transcribe(temp_audio_path)
            print(f"Whisper result: {result}")
            return {"text": result["text"]}
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if temp_audio_path and os.path.exists(temp_audio_path):
                try:
                    os.unlink(temp_audio_path)
                except Exception as e:
                    print(f"Failed to delete temp file: {e}")
    except Exception as e:
        print(f"STT Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def remove_tts_code():
    pass

if __name__ == "__main__":
    import uvicorn
    # ì„œë²„ ì‹œì‘ ì‹œ debug.log íŒŒì¼ ì´ˆê¸°í™”
    with open("debug.log", "w", encoding="utf-8") as f:
        f.write("[DEBUG] ì„œë²„ê°€ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤!\n")
        f.write(f"[DEBUG] ì‹¤í–‰ ì¤‘ì¸ íŒŒì¼: {os.path.abspath(__file__)}\n")
    
    # Static íŒŒì¼ ë§ˆìš´íŠ¸
    app.mount("/", StaticFiles(directory=".", html=True), name="static")
    
    uvicorn.run(app, host="0.0.0.0", port=8000) 